{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbdc7c-e470-44e3-b411-bcc3fb4a4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d302f0-e2a8-4570-b9b0-7ac9407db07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e627d4-284b-4b58-9e83-7e50dc980eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8ba0f2-f23b-40e9-b8c9-670122d8b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import numpy as np\n",
    "\n",
    "# Iniciar Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark PDF Indexing\") \\\n",
    "    .config(\"spark.es.nodes\", \"elasticsearch\") \\\n",
    "    .config(\"spark.es.port\", \"9200\") \\\n",
    "    .config(\"spark.es.nodes.wan.only\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6fd0e44-d8f3-4200-8199-dac5e9143e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|       sentence_text|\n",
      "+---+--------------------+\n",
      "|  0|MuHeQA: Zero-shot...|\n",
      "|  1|First, the approa...|\n",
      "|  2|A novel algorithm...|\n",
      "|  3|IntroductionKnowl...|\n",
      "|  4|   knowledge graphs.|\n",
      "|  5|KGQA systems do n...|\n",
      "|  6|                  ].|\n",
      "|  7|In this step,natu...|\n",
      "|  8|Once the template...|\n",
      "|  9|Thus,KGQA systems...|\n",
      "| 10|This approach is ...|\n",
      "| 11|Moreover,the depe...|\n",
      "| 12|The combined use ...|\n",
      "| 13|Thus, our second ...|\n",
      "| 14|And finally, anot...|\n",
      "| 15|no need to create...|\n",
      "| 16|MuHeQAsupportssin...|\n",
      "| 17|textual content a...|\n",
      "| 18|Section 2 present...|\n",
      "| 19|Our proposal is d...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Función para extraer texto de un PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:  # Asegurar que no se añada texto nulo\n",
    "                new_content = page_text.replace(\"\\n\", \"\").strip() + \" \"\n",
    "                if (len(new_content)>10):\n",
    "                    text += new_content\n",
    "    return text\n",
    "\n",
    "# Función para dividir el texto en oraciones\n",
    "def chunk_text_by_sentences(text):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    return tokenizer.tokenize(text.strip())\n",
    "\n",
    "# Ruta al PDF\n",
    "pdf_path = \"paper.pdf\"\n",
    "\n",
    "# Extraer y segmentar el texto del PDF en oraciones\n",
    "full_text = extract_text_from_pdf(pdf_path)\n",
    "sentences = chunk_text_by_sentences(full_text)\n",
    "\n",
    "# Crear un DataFrame con las oraciones\n",
    "sentences_data = [(i, sentence) for i, sentence in enumerate(sentences)]\n",
    "df = spark.createDataFrame(sentences_data, [\"id\", \"sentence_text\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4753e570-5ca3-4165-a0e0-121637b83c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificar el índice de Elasticsearch donde se indexarán los datos\n",
    "index_name = \"pdf_sentences\"\n",
    "\n",
    "# Escribir el DataFrame a Elasticsearch\n",
    "df.write.format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.resource\", f\"{index_name}/_doc\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "# Detener la sesión Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
